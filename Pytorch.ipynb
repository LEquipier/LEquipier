{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a56d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [0/60000 (0%)]\t loss: 2.320711\t train accuracy: 6.25%\t right rate: 16.38%\n",
      "epoch: 0 [6400/60000 (11%)]\t loss: 0.260307\t train accuracy: 73.53%\t right rate: 92.47%\n",
      "epoch: 0 [12800/60000 (21%)]\t loss: 0.259206\t train accuracy: 83.61%\t right rate: 95.38%\n",
      "epoch: 0 [19200/60000 (32%)]\t loss: 0.089080\t train accuracy: 87.55%\t right rate: 96.79%\n",
      "epoch: 0 [25600/60000 (43%)]\t loss: 0.283640\t train accuracy: 89.73%\t right rate: 96.52%\n",
      "epoch: 0 [32000/60000 (53%)]\t loss: 0.062103\t train accuracy: 91.12%\t right rate: 97.39%\n",
      "epoch: 0 [38400/60000 (64%)]\t loss: 0.018309\t train accuracy: 92.11%\t right rate: 97.53%\n",
      "epoch: 0 [44800/60000 (75%)]\t loss: 0.127803\t train accuracy: 92.84%\t right rate: 97.89%\n",
      "epoch: 0 [51200/60000 (85%)]\t loss: 0.048107\t train accuracy: 93.44%\t right rate: 98.08%\n",
      "epoch: 0 [57600/60000 (96%)]\t loss: 0.061304\t train accuracy: 93.91%\t right rate: 97.93%\n",
      "epoch: 1 [0/60000 (0%)]\t loss: 0.044280\t train accuracy: 98.44%\t right rate: 98.06%\n",
      "epoch: 1 [6400/60000 (11%)]\t loss: 0.290502\t train accuracy: 98.34%\t right rate: 98.27%\n",
      "epoch: 1 [12800/60000 (21%)]\t loss: 0.030049\t train accuracy: 98.08%\t right rate: 98.29%\n",
      "epoch: 1 [19200/60000 (32%)]\t loss: 0.029336\t train accuracy: 98.11%\t right rate: 98.43%\n",
      "epoch: 1 [25600/60000 (43%)]\t loss: 0.065846\t train accuracy: 98.16%\t right rate: 98.36%\n",
      "epoch: 1 [32000/60000 (53%)]\t loss: 0.018534\t train accuracy: 98.18%\t right rate: 98.52%\n",
      "epoch: 1 [38400/60000 (64%)]\t loss: 0.038727\t train accuracy: 98.17%\t right rate: 98.58%\n",
      "epoch: 1 [44800/60000 (75%)]\t loss: 0.044650\t train accuracy: 98.17%\t right rate: 98.50%\n",
      "epoch: 1 [51200/60000 (85%)]\t loss: 0.049488\t train accuracy: 98.18%\t right rate: 98.69%\n",
      "epoch: 1 [57600/60000 (96%)]\t loss: 0.056067\t train accuracy: 98.24%\t right rate: 98.61%\n",
      "epoch: 2 [0/60000 (0%)]\t loss: 0.014777\t train accuracy: 100.00%\t right rate: 98.33%\n",
      "epoch: 2 [6400/60000 (11%)]\t loss: 0.100925\t train accuracy: 98.70%\t right rate: 98.68%\n",
      "epoch: 2 [12800/60000 (21%)]\t loss: 0.211728\t train accuracy: 98.82%\t right rate: 98.79%\n",
      "epoch: 2 [19200/60000 (32%)]\t loss: 0.065803\t train accuracy: 98.71%\t right rate: 98.51%\n",
      "epoch: 2 [25600/60000 (43%)]\t loss: 0.055402\t train accuracy: 98.67%\t right rate: 98.73%\n",
      "epoch: 2 [32000/60000 (53%)]\t loss: 0.077994\t train accuracy: 98.75%\t right rate: 98.76%\n",
      "epoch: 2 [38400/60000 (64%)]\t loss: 0.009651\t train accuracy: 98.72%\t right rate: 98.67%\n",
      "epoch: 2 [44800/60000 (75%)]\t loss: 0.074992\t train accuracy: 98.70%\t right rate: 98.73%\n",
      "epoch: 2 [51200/60000 (85%)]\t loss: 0.043632\t train accuracy: 98.70%\t right rate: 98.75%\n",
      "epoch: 2 [57600/60000 (96%)]\t loss: 0.067301\t train accuracy: 98.73%\t right rate: 98.94%\n",
      "111.37630414962769 s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as py\n",
    "import time\n",
    "\n",
    "##%matplotlib inline\n",
    "\n",
    "input_size = 28\n",
    "num_classes = 10\n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "##制作训练集和测试集\n",
    "train_dataset = datasets.MNIST(root='./DATA', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='DATA', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "##制作batch数据集\n",
    "train_load = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2))\n",
    "\n",
    "        ##self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2))\n",
    "\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "        ##self.out = nn.Linear(64 * 3 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        ##x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    pred = torch.max(predictions.data, 1)[1]\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum()\n",
    "    return rights, len(labels)\n",
    "\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_rights = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_load):\n",
    "        net.train()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        right = accuracy(output, target)\n",
    "        train_rights.append(right)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            net.eval()\n",
    "            val_rights = []\n",
    "\n",
    "            for (data, target) in test_load:\n",
    "                output = net(data)\n",
    "                right = accuracy(output, target)\n",
    "                val_rights.append(right)\n",
    "            train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))\n",
    "            val_r = (sum([tup[0] for tup in val_rights]), sum([tup[1] for tup in val_rights]))\n",
    "\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t loss: {:.6f}\\t train accuracy: {:.2f}%\\t right rate: {:.2f}%'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_load.dataset),\n",
    "                       100. * batch_idx / len(train_load),\n",
    "                loss.data,\n",
    "                       100. * train_r[0].numpy() / train_r[1],\n",
    "                       100. * val_r[0].numpy() / val_r[1]))\n",
    "time_end = time.time()\n",
    "\n",
    "print(time_end - time_begin,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bf0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7a9f7dfaf44e5795891a9354266d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1973fb89f344379628616b5c15835b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5143deb839049bfbaf914a6aaf6b3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7985d0fb94d42b4b7a193a319b1a570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "])#把[]中的操作整成一个pipline,均值和标准差\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./dataset/mnist/',\n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transform)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(root='./dataset/mnist/',\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=transform)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         shuffle=False,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(320, 10)\n",
    "    def forward(self, x):\n",
    "    # Flatten data from (n, 1, 28, 28) to (n, 784)\n",
    "        batch_size = x.size(0)\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        x = x.view(batch_size, -1)  # flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        print('Accuracy on test set: %d %%' % (100 * correct / total))#%%输出就是%，转义\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44968f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
